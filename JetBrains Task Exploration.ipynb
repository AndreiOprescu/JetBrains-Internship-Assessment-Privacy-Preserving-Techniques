{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiUyhYtPd6qd"
   },
   "source": [
    "# Privacy-Preserving Techniques for LLM Code Completion: Andrei Oprescu\n",
    "\n",
    "This jupyter notebook is dedicated to completing the task for the Privacy-Preserving Techniques for LLM Code Completion JetBrains task. Below you can find the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcdsBgRyexKg"
   },
   "source": [
    "First, lets install the dependencies and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vVlwqSJ3SSkx",
    "outputId": "772a8a68-c831-4fb9-9ad2-a20c5724094f"
   },
   "source": [
    "%pip install transformers torch\n",
    "%pip install pandas\n",
    "%pip install datasets\n",
    "%pip install numpy\n",
    "%pip install rouge-score\n",
    "%pip install matplotlib\n",
    "%pip install accelerate\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XS98Da11Upte"
   },
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvU1s_Bai3c7"
   },
   "source": [
    "### Data Fetching and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHKSwXPpe8LZ"
   },
   "source": [
    "Next, we will get the dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2p2CvVNUdiT",
    "outputId": "3e3d90dc-009d-4038-f88e-9cb3197b8bfd"
   },
   "source": [
    "dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "dataset.set_format(\"pandas\")\n",
    "print(\"Length: \", len(dataset))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw7hIjGWfOqs"
   },
   "source": [
    "Get the first 20 examples for the code completion and print one out."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxbuoy0KUhec",
    "outputId": "ff342ec5-ac7c-45de-ff40-8b3b096b5e77"
   },
   "source": [
    "first20 = dataset[:20][[\"task_id\", \"prompt\", \"canonical_solution\"]]\n",
    "print(first20.iloc[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wshx-exfmX5"
   },
   "source": [
    "For the low obfuscation version, I decided to remove the variable and method data types. This should not affect the code completion model's ability to understand the task too much, since it can still make out what the variable and method are supposed to be used for from the method name, comments, and variable name."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UigsFX6sUkJ8"
   },
   "source": [
    "def remove_types(match):\n",
    "    # Clean the signature string found by the main regex\n",
    "    signature = match.group(0)\n",
    "    # Remove param types by looking for the colon\n",
    "    signature = re.sub(r\":\\s*[^,)=]+\", \"\", signature)\n",
    "    # Remove return type by looking for the ->\n",
    "    signature = re.sub(r\"->\\s*[^:]+\", \"\", signature)\n",
    "    return signature\n",
    "\n",
    "def low_obfuscation(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    # Define the method signature regex expression\n",
    "    signature_pattern = r\"def\\s+\\w+\\s*\\(.*?\\)\\s*->\\s*[^:]+:|def\\s+\\w+\\s*\\(.*?\\):\"\n",
    "    # Make a copy to not edit the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Make helper to handle regex for each prompt\n",
    "    def process_prompt(text):\n",
    "        return re.sub(signature_pattern, remove_types, text)\n",
    "                      \n",
    "    # Create new column \"edited_prompt\" with apply and helper\n",
    "    df_copy['edited_prompt'] = df_copy[\"prompt\"].apply(process_prompt)\n",
    "    return df_copy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjywhR2KhdV8"
   },
   "source": "Now let's create the low-obfuscated prompts and save them."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmFTz399Xt99",
    "outputId": "4db5d547-52d4-4807-bae2-2349f1ec2cad"
   },
   "source": [
    "# Get the low-obfuscated prompts and print one\n",
    "first20_low_obfuscation = low_obfuscation(first20)\n",
    "print(first20_low_obfuscation.iloc[1][\"edited_prompt\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKvUIm9vhp7X"
   },
   "source": "For the high-obfuscation, I decided to just remove the single-line and multi-line comments from the method. This should make it harder for the code completion model to understand how to autocomplete the code, since pretty much the only piece of information it has about its task is the method signature."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZmqKwEbnUk2M"
   },
   "source": [
    "def high_obfuscation(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    # Define the multi-line and single line regex expressions\n",
    "    doc_pattern = r\"\\\"\\\"\\\".*?\\\"\\\"\\\"|'''.*?'''\"\n",
    "    inline_pattern = r\"#.*?$\"\n",
    "    # Make a copy of the DataFrame to not override the prompts from the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Make helper for removing comments\n",
    "    def clean_comments(text):\n",
    "        text = re.sub(doc_pattern, \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(inline_pattern, \"\", text, flags=re.MULTILINE)\n",
    "        return text\n",
    "\n",
    "    # Create new column \"edited_prompt\" with apply and helper\n",
    "    df_copy['edited_prompt'] = df_copy[\"prompt\"].apply(clean_comments)\n",
    "\n",
    "    return df_copy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUGYcIT2jEqp"
   },
   "source": [
    "Create the high-obfuscated prompts and save them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPfnlt2zX1ia",
    "outputId": "e4dd94c5-9c7e-4bb9-d3b8-c3af23d71142"
   },
   "source": [
    "first20_high_obfuscation = high_obfuscation(first20)\n",
    "print(first20_high_obfuscation.iloc[1][\"edited_prompt\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's also make the 'edited_prompt' column for the original prompt df for consistency."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "first20['edited_prompt'] = first20['prompt']\n",
    "print(first20.iloc[1][\"edited_prompt\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIylkhsGi1aF"
   },
   "source": [
    "### Code Autocompletion\n",
    "\n",
    "Next, we will get a model from HuggingFace to autocomplete the method functionality. I chose a model with less parameters, since the obfuscation methods should have a higher effect on its performance since it's harder for it to analyse the task, allowing us to see a bigger effect of these obfuscation methods. Additionally, this improved the download speed of the model as well as inference times. The model parameter count can be changed by you by changing the \"3B\" part of the model_name to any of the following values:  0.5B, 1.5B, 3B, 7B, 14B, 32B"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "fea435ad14d3472baba826c7966d8dc0",
      "2c106fba6cf54b9e993366f8b9070b2e",
      "a6ac3cbea2004113a3e23af92a24189a",
      "769d1c47e5bf498a8d696d4cf5b44246",
      "9fe45ce573874ca0869858b23d22e0bb",
      "bc64168a27794f13a013652b1d06f1ff",
      "26fae799322e42e19a9a54a5a8da1301",
      "87fe6fda46574cd4904028825b019ac5",
      "ea551714fdbe43c182374267756e0efb",
      "51e82dd77049435b8993950d2e7a719e",
      "72e641a9354049e8a7fb79acbf9eaa1b"
     ]
    },
    "id": "-BoMTzkmQaUc",
    "outputId": "5158854f-c421-4d75-cdd2-662daeafa538"
   },
   "source": [
    "# Choose a low-parameter model (1.5B parameters)\n",
    "model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: \", device)\n",
    "\n",
    "# Load tokenizer and model and use the GPU as the device\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtwTXOo0kvgF"
   },
   "source": [
    "Next, I created a method for completing the code of the 20 prompts in the DataFrame. Since I gave the model no other information, I defined a list of stop strings. The model will stop writing more code once it writes these keywords. Namely, it will stop if it tries to create another method, class or run the code it created in an \"if \\_\\_name\\_\\_ == '\\_\\_init\\_\\_':\" statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gynnbc3gMvkV"
   },
   "source": [
    "stop_strings = [\"\\ndef\", \"\\nclass\", \"\\nif __name__\"]\n",
    "\n",
    "def complete_code(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "  # Store the completions\n",
    "  completions = []\n",
    "\n",
    "  for _, row in df.iterrows():\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(row['edited_prompt'], return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    # Generate the output of the model for the prompt\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        tokenizer=tokenizer,\n",
    "        stop_strings=stop_strings,\n",
    "        temperature=0.1, # Lower temperature for more deterministic and accurate code\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decode and store the result\n",
    "    completion_tokens = outputs[0][prompt_length:]\n",
    "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
    "    completions.append((completion, row['canonical_solution']))\n",
    "\n",
    "  return completions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tgyQUhjl00O"
   },
   "source": [
    "Next, let's get the code completions of the original, low-obfuscated and high-obfuscated prompts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "lBdyrR18Tfhp"
   },
   "source": [
    "first20_completions = complete_code(first20)\n",
    "first20_low_obfuscation_completions = complete_code(first20_low_obfuscation)\n",
    "first20_high_obfuscation_completions = complete_code(first20_high_obfuscation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnvsm1bLmMtr"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "I first created the methods for normalized Levenshtein for privacy score and ROUGE score for utility score. I implemented the normalized Levenshtein algorithm myself. For the ROUGE scores, I decided to use a library which calculates them itself.\n",
    "\n",
    "For the ROUGE score, I decided to look at the longest common subsequence (ROUGE-L). I retrieved the precisions, recalls and F-scores of the completions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "77QU4FlUOVXq"
   },
   "source": [
    "def normalized_levenshtein(str1: str, str2: str) -> float:\n",
    "  # Get lengths\n",
    "  l1 = len(str1)\n",
    "  l2 = len(str2)\n",
    "\n",
    "  # Create the memoization 2-D array\n",
    "  mem = np.zeros((l1 + 1, l2 + 1))\n",
    "\n",
    "  # Fill in the base values for both words\n",
    "  for i in range(l1 + 1):\n",
    "    mem[i][0] = i\n",
    "\n",
    "  for j in range(l2 + 1):\n",
    "    mem[0][j] = j\n",
    "\n",
    "  # Fill in the matrix\n",
    "  for i in range(1, l1 + 1):\n",
    "    for j in range(1, l2 + 1):\n",
    "      if str1[i - 1] == str2[j - 1]:\n",
    "        # If the characters match\n",
    "        mem[i][j] = mem[i - 1][j - 1]\n",
    "      else:\n",
    "        # If the characters don't match\n",
    "        mem[i][j] = 1 + max(mem[i - 1][j], max(mem[i][j - 1], mem[i - 1][j - 1]))\n",
    "\n",
    "  # Get the distance between the two words\n",
    "  distance = mem[l1][l2]\n",
    "\n",
    "  # Return the normalized distance\n",
    "  return distance / max(l1, l2)\n",
    "\n",
    "\n",
    "def perform_levenshtein(df: pandas.DataFrame) -> int:\n",
    "  \"\"\"\n",
    "    Perform normalized Levenshtein distance on completions-solution tuple pairs.\n",
    "    Return the average Levenshtein distance.\n",
    "  \"\"\"\n",
    "  total = 0\n",
    "  distances = []\n",
    "  for _, row in df.iterrows():\n",
    "    distance = normalized_levenshtein(row['prompt'], row['edited_prompt'])\n",
    "    distances.append(distance)\n",
    "    total += distance\n",
    "\n",
    "  return distances\n",
    "\n",
    "def get_rouge_score(completions):\n",
    "  \"\"\"\n",
    "    Get the ROUGE score of the completion-solution tuple pairs.\n",
    "    Return precisions, recalls and f-measures.\n",
    "  \"\"\"\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  fmeasures = []\n",
    "\n",
    "  n = len(completions)\n",
    "\n",
    "  for completion, solution in completions:\n",
    "    # Create the ROUGE scorer and get the score for the completion\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(completion, solution)\n",
    "\n",
    "    # Store the results\n",
    "    for key, value in scores.items():\n",
    "        print(key, value)\n",
    "        precisions.append(value.precision)\n",
    "        recalls.append(value.recall)\n",
    "        fmeasures.append(value.fmeasure)\n",
    "\n",
    "  # Return the precision, recall and F-measure scores\n",
    "  return precisions, recalls, fmeasures\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-owAK7BCoeJ0"
   },
   "source": [
    "Next, let's get the scores for the original, low-obfuscated and high-obfuscated prompts and group them by score type. I averaged the measures so that the plot can show the big picture of the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fIahBu98T6Mb"
   },
   "source": [
    "# Get the scores\n",
    "f20_precisions, f20_recalls, f20_fmeasures = get_rouge_score(first20_completions)\n",
    "low_obfuscation_precisions, low_obfuscation_recalls, low_obfuscation_fmeasures = get_rouge_score(first20_low_obfuscation_completions)\n",
    "high_obfuscation_precisions, high_obfuscation_recalls, high_obfuscation_fmeasures = get_rouge_score(first20_high_obfuscation_completions)\n",
    "\n",
    "# Group by score type\n",
    "precisions = [np.average(f20_precisions), np.average(low_obfuscation_precisions), np.average(high_obfuscation_precisions)]\n",
    "recalls = [np.average(f20_recalls), np.average(low_obfuscation_recalls), np.average(high_obfuscation_recalls)]\n",
    "fmeasures = [np.average(f20_fmeasures), np.average(low_obfuscation_fmeasures), np.average(high_obfuscation_fmeasures)]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXv6sc7UpB40"
   },
   "source": [
    "Finally, I created a method for plotting these scores on a bar graph to showcase the difference in code completion accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d_8qBuKRWyec"
   },
   "source": [
    "def plot_scores(precisions, recalls, fmeasures):\n",
    "  # Define the plot categories\n",
    "  categories = [\"No obfuscation\", \"Low Obfuscation\", \"High Obfuscation\"]\n",
    "  x = np.arange(len(categories))\n",
    "  # Set the width of the bars\n",
    "  width = 0.2\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # Plot the bars with manual offsets\n",
    "  ax.bar(x - width, precisions, width, label='Precision', color='yellow', edgecolor='grey')\n",
    "  ax.bar(x, recalls, width, label='Recall', color='blue', edgecolor='grey')\n",
    "  ax.bar(x + width, fmeasures, width, label='F-Score', color='green', edgecolor='grey')\n",
    "\n",
    "  ax.set_ylabel('Scores')\n",
    "  ax.set_title('Grouped Bar Chart Example')\n",
    "  ax.set_xticks(x)\n",
    "  ax.set_xticklabels(categories)\n",
    "  ax.legend()\n",
    "\n",
    "  # Plot the bar graph\n",
    "  plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp0HE9u1q2tf"
   },
   "source": "Next, let's print out the ROUGE scores in a table and plot the graph. I also displayed the standard deviation of the scores so that we can see how much the utility scores vary with the obfuscations."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wLte77oiqEru"
   },
   "source": [
    "# Plot the scores\n",
    "plot_scores(precisions, recalls, fmeasures)\n",
    "\n",
    "# Define the data for the table\n",
    "data = {\n",
    "    'Original': [np.average(f20_precisions), np.average(f20_recalls), np.average(f20_fmeasures)],\n",
    "    'Low Obfuscation': [np.average(low_obfuscation_precisions), np.average(low_obfuscation_recalls), np.average(low_obfuscation_fmeasures)],\n",
    "    'High Obfuscation': [np.average(high_obfuscation_precisions), np.average(high_obfuscation_recalls), np.average(high_obfuscation_fmeasures)]\n",
    "}\n",
    "\n",
    "# Define the row labels\n",
    "row_titles = ['Precision', 'Recall', 'F-Score']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pandas.DataFrame(data, index=row_titles)\n",
    "\n",
    "print(df)\n",
    "print(f\"Original prompt F-measure standard deviation: {np.std(f20_fmeasures)}\")\n",
    "print(f\"Low obfuscation prompt F-measure standard deviation: {np.std(low_obfuscation_fmeasures)}\")\n",
    "print(f\"High obfuscation prompt F-measure standard deviation: {np.std(high_obfuscation_fmeasures)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmLjMJWyqoLd"
   },
   "source": [
    "As we can see from the plot and the table, the less obfuscated code is autocompleted by the code completion model with a high accuracy, achieving an F-Score similar to the original prompt. However, the F-score can also be higher for the low obfuscated prompt than the original prompt. This most likely happens because of the uncertainty of the model; since the obfuscation does not greatly sabotage the model's ability to understand the task to a great extent, it might end up producing better results on occasional cases, but should not end up being overall better if we used more prompts to assess the effect of obfuscation on utility.\n",
    "\n",
    "However, when we obfuscate the code more (by removing the comments), the task given to the model is less obvious, which causes the F-score to drop substantially. This is expected, as the model has less information to work with.\n",
    "\n",
    "Therefore, we see the utility of the code to drop as we increase the obfuscation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p_SSmbBIL1f"
   },
   "source": "Now let's get the levenshtein distances for each version of the prompts."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PUSV5jpkVbjg"
   },
   "source": [
    "levenshtein_first20 = perform_levenshtein(first20)\n",
    "levenshtein_first20_low_obfuscation = perform_levenshtein(first20_low_obfuscation)\n",
    "levenshtein_first20_high_obfuscation = perform_levenshtein(first20_high_obfuscation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c6jmZn6EWRDi"
   },
   "source": [
    "print(\"Original prompt Levenshtein score (average): \", np.average(levenshtein_first20))\n",
    "print(\"Low-obfuscation prompt Levenshtein score (average): \", np.average(levenshtein_first20_low_obfuscation))\n",
    "print(\"High-obfuscation prompt Levenshtein score (average): \", np.average(levenshtein_first20_high_obfuscation))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaWb_AyHKo5_"
   },
   "source": [
    "From the normalized Levenshtein distances, we can see that a low-obfuscation of the code affects the privacy very little. However, when we apply a high-obfuscation method, the normalized Levenshtein distances get bigger by a considerable amount.\n",
    "\n",
    "\n",
    "Next, let's plot the Utility score on the Privacy score in a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8lvAKluqYkqY"
   },
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Original Data\n",
    "plt.scatter(levenshtein_first20, f20_fmeasures, color='green', label='Original', alpha=0.6)\n",
    "\n",
    "# Plot Low Obfuscation\n",
    "plt.scatter(levenshtein_first20_low_obfuscation, low_obfuscation_fmeasures, color='yellow', label='Low Obfuscation',\n",
    "            alpha=0.8, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Plot High Obfuscation\n",
    "plt.scatter(levenshtein_first20_high_obfuscation, high_obfuscation_fmeasures, color='red', label='High Obfuscation', alpha=0.8)\n",
    "\n",
    "all_x = np.concatenate([levenshtein_first20, levenshtein_first20_low_obfuscation, levenshtein_first20_high_obfuscation])\n",
    "all_y = np.concatenate([f20_fmeasures, low_obfuscation_fmeasures, high_obfuscation_fmeasures])\n",
    "\n",
    "# Calculate the line of best fit\n",
    "coefficients = np.polyfit(all_x, all_y, 1)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Make a range of x values for the line of best fit\n",
    "x_line = np.linspace(all_x.min(), all_x.max(), 100)\n",
    "y_line = polynomial(x_line)\n",
    "\n",
    "plt.plot(x_line, y_line, color='purple', linestyle='--', linewidth=2, label=f'Trend (Best Fit)')\n",
    "\n",
    "# Adding labels and styling\n",
    "plt.xlabel('Privacy Score (Normalized Levenshtein Distance)')\n",
    "plt.ylabel('Utility Score (ROUGE Score)')\n",
    "plt.title('Privacy-Utility Trade-off in Code Completion')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3KqKa9BvzeU"
   },
   "source": "From the plot, we can see that as the Privacy score increases, the Utility score decreases, as shown by the trend line. As mentioned before, this makes sense, as the more information we remove from the prompt (the higher the privacy), the less useful answers we receive (the lower the utility). Additionally, the red points (highly obfuscated prompts) tend to be lower on the utility score and higher on the normalized Levenshtein score compared to the yellow and green points (less obfuscated and original prompts)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26fae799322e42e19a9a54a5a8da1301": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c106fba6cf54b9e993366f8b9070b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc64168a27794f13a013652b1d06f1ff",
      "placeholder": "​",
      "style": "IPY_MODEL_26fae799322e42e19a9a54a5a8da1301",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "51e82dd77049435b8993950d2e7a719e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72e641a9354049e8a7fb79acbf9eaa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "769d1c47e5bf498a8d696d4cf5b44246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51e82dd77049435b8993950d2e7a719e",
      "placeholder": "​",
      "style": "IPY_MODEL_72e641a9354049e8a7fb79acbf9eaa1b",
      "value": " 4/4 [00:00&lt;00:00,  3.47it/s]"
     }
    },
    "87fe6fda46574cd4904028825b019ac5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fe45ce573874ca0869858b23d22e0bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ac3cbea2004113a3e23af92a24189a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87fe6fda46574cd4904028825b019ac5",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea551714fdbe43c182374267756e0efb",
      "value": 4
     }
    },
    "bc64168a27794f13a013652b1d06f1ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea551714fdbe43c182374267756e0efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fea435ad14d3472baba826c7966d8dc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c106fba6cf54b9e993366f8b9070b2e",
       "IPY_MODEL_a6ac3cbea2004113a3e23af92a24189a",
       "IPY_MODEL_769d1c47e5bf498a8d696d4cf5b44246"
      ],
      "layout": "IPY_MODEL_9fe45ce573874ca0869858b23d22e0bb"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
